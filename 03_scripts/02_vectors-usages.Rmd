---
title: Working with vectors    
author: Erika Duan  
date: "`r Sys.Date()`"  
output: 
  github_document:
    toc: TRUE
    pandoc_args: --webtex  
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = 'hide', fig.show = 'hold', fig.align = 'center')
knitr::knit_engines$set(python = reticulate::eng_python)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}  
#-----load required R packages-----
if (!require("pacman")) install.packages("pacman") 
p_load(tidyverse,
       reticulate,
       knitr) 

conda_list() # list all available conda environments
use_condaenv("Anaconda3")

py_run_string("import os as os")
py_run_string("os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/user/Anaconda3/Library/plugins/platforms'")
```


# Resources   

This section on vectors is taken from [Introduction to Linear Algebra for Applied Machine Learning with Python](https://pabloinsente.github.io/intro-linear-algebra#vector-norms) by Pablo Caceres and [Lecture 8 - Norms of Vectors and Matrices](https://www.youtube.com/watch?v=NcPUI7aPFhA) from  Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, MIT OpenCourseWare. All credit should be attributed to these sources.        


# Vector norms  

To measure vectors, you can think about the norm (i.e. length) of the vector as the distance between its origin and end points.     

$\lVert x \rVert = \sqrt{x_1^2 + x_2^2+...+x_n^2}$   

Norms are functions that map vectors to non-negative values. They are used to assign $length\; \lVert x\rVert \in {\rm I\!R}^n$ to a vector x.      

A norm has to satisfy the following three properties:  

+ It is absolutely homogenous. For all scalars that are real values, the length scales proportionally with the value of the scalar. $\forall \alpha \in {\rm I\!R}, \; \lVert \alpha x \rVert = \lVert \alpha \rVert \cdot \lVert x \rVert$           
+ It exhibits triangular inequality. In geometric terms, the sum of the lengths of any two sides must be greater or equal to the length of the third side. $\lVert x+y \rVert \leq \lVert x \rVert + \lVert y \rVert$.            
+ It is positively defined. The length of x has to be a positive value and a length of zero only occurs if x = 0. $\lVert x \rVert \geq 0 \; and \; \lVert x \rVert = 0 \iff x = 0$.       

The three main types of vector norms are illustrated below.  

```{r, echo = FALSE, results = 'markup', fig.align = 'center', out.width = '80%'}
knitr::include_graphics("../02_figures/02_vectors-norm-types.jpg")   
```


## Euclidean norm    

The Euclidean or $L_2$ norm is defined below. 
$$\lVert x\rVert_2 := \sqrt{\displaystyle\sum_{i=1}^{n} x_i^2} = \sqrt{x^T\times x} $$

The euclidean norm can also be thought of as the square root of the dot product of a vector by itself.  

In 2D, the Eucliean norm is $\lVert x\rVert_2 \in {\rm I\!R}^2 = \sqrt{x_1^2 + x_2^2}$.   
This is equivalent to the Pythagoras theorem for calculating the hypotenuse of a triangle with sides $x_1^2$ and $x_2^2$.   

```{python}
#-----calculate L2 norm in Python via NumPy-----  
import numpy as np  

x = np.array([[3],
              [-4]])  
              
np.linalg.norm(x, 2) 

#> 5.0  
```


## Manhattan norm   

The Manhattan or $L_1$ norm is analogous to measuring distances whilst moving in only vertical and horizontal lines. It is defined below.    
$$\lVert x\rVert_1 := \displaystyle\sum_{i=1}^{n}|x_i|$$   

The manhattan norm is preferred for discriminating between elements that are exactly zero and elements that are small but not zero (or for calculating a distance matrix between categorical data points).    

```{python}
#-----calculate L1 norm in Python via NumPy-----  
x = np.array([[3],
              [-4]])  
              
np.linalg.norm(x, 1) 

#> 7.0 

# |3| + |-4| = 7  
```


## Max norm     

The Max norm or infinity norm is the absolute value of the largest element in the vector. It is defined below.  
$$\lVert x\rVert_\infty := max_i|x_i|$$   

The max norm is a useful norm type as when the number of dimensions increases, the vector length will be disproportionately influenced by the largest vector shown from the equation below.    
$$\lVert x\rVert_p := \sqrt{\displaystyle\sum_{i=1}^{n} x_i^p}$$      

```{python}
#-----calculate max norm in Python via NumPy-----  
x = np.array([[3],
              [-4]])  
              
np.linalg.norm(x, np.inf) 

#> 4.0 
```


# Vector inner product, length and distance    

For practicality, you can conside the vector inner product and vector length as equivalent to the vector dot product and vector norm.    
Every dot product is an inner product. The scalar inner product is defined below.  
$$\langle x, y\rangle := x \cdot y$$  

In ${\rm I\!R}^n$, the inner product is a dot product defined below.  
$$\langle 
\begin{bmatrix}
  x_1 \\
  \vdots \\
  x_n
\end{bmatrix},
\begin{bmatrix}
  y_1 \\
  \vdots \\
  y_n
\end{bmatrix}
\rangle := x \cdot y = \displaystyle\sum_{i=1}^{n} x_i\times y_i$$  

Because length is a concept from geometry, you can say that geometric vectors have lengths and that vectors in ${\rm I\!R}^n$ have norms.   

Distance is therefore a relational concept, as it refers to the length (or norm) of the difference between two vectors. For example, consider the vectors x and y. We define the distance $d(x, y)$ as below.  
$$d(x, y) := \lVert x - y \rVert = \sqrt{\langle x-y, x -y \rangle}$$  

This definition for distance is flexible. When the inner product is specifically the dot product, the distance equals to the Euclidean distance.   
```{r, echo = FALSE, results = 'markup', fig.align = 'center', out.width = '80%'}
knitr::include_graphics("../02_figures/02_vectors-distance.jpg")   
```

```{python}
#-----calculate L2 distance between two vectors in Python via NumPy-----  
x = np.array([[3],
              [-4]])   

y = np.array([[-2],
              [2]])

np.transpose(x) @ y 
#> array([[-14]])  

distance = np.linalg.norm(x-y, 2)

print("L2 distance between x and y: {}".format(distance))
#> L2 distance between x and y: 7.810249675906654  
```


# Vector angles and orthogonality  

A brief revision of angles is shown below.  

```{r, echo = FALSE, results = 'markup', fig.align = 'center', out.width = '100%'}
knitr::include_graphics("../02_figures/02_vectors-angles-revision-1.jpg")   
```

In particular, the **Cosine rule** is particularly useful for calculating vector similarity.     

```{r, echo = FALSE, results = 'markup', fig.align = 'center', out.width = '100%'}
knitr::include_graphics("../02_figures/02_vectors-angles-revision-2.jpg")   
```


# Systems of linear equations     

```{r}

```

