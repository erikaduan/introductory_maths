---
title: Cosine similarity matrix       
author: Erika Duan  
date: "`r Sys.Date()`"  
output: 
  github_document:
    toc: TRUE
    pandoc_args: --webtex  
---

# Cosine similarity  

In machine learning, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is a similarity measurement between two non-zero vectors that is equal to the cosine of the angle between them. This is the same as calculating the inner product of two vectors normalised to have norms of 1 (i.e. cosine similarity only cares about vector direction and not magnitude).    

$similarity = \cos\theta=\frac{\langle x, y\rangle}{\lVert x\rVert \lVert y \rVert} = \frac{\displaystyle\sum_{i=1}^nA_iB_i}{\sqrt{\displaystyle\sum_{i=1}^nA^2_i} \times \sqrt{\displaystyle\sum_{i=1}^nB^2_i}}$   

In text mining, each unique term is assigned a different dimension, so cosine similarity calculations tend to be applied to very high dimensions. A document is then viewed as a vector whose direction is determined by the proportion of unique terms that it contains.     

```{r, echo = FALSE, results = 'markup', fig.align = 'center', out.width = '90%'}
knitr::include_graphics("../02_figures/02_vectors-angles-revision-7.jpg")   
```

```{r}
# Calculate cosine similarity in R ---------------------------------------------   



Matrix <- as.matrix(DF)
sim <- Matrix / sqrt(rowSums(Matrix * Matrix))
sim <- sim %*% t(sim)
```

```{python}
# Calculate cosine similarity in Python via sklearn ---------------------------- 
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics.pairwise import cosine_similarity

documents = ["learning Python and R are not so hard",
             "do I need to learn Python to be a data scientist",
             "R is my favourite data science language",
             "I use excel spreadsheets"]  
             
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents) # TF-IDF sparse matrix

doc_term_matrix = tfidf_matrix.todense()
doc_term_df = pd.DataFrame(doc_term_matrix,
                           columns = tfidf_vectorizer.get_feature_names())

doc_term_df
#>         and       are        be  ...  spreadsheets        to      use
#> 0  0.388614  0.388614  0.000000  ...       0.00000  0.000000  0.00000
#> 1  0.000000  0.000000  0.312451  ...       0.00000  0.624903  0.00000
#> 2  0.000000  0.000000  0.000000  ...       0.00000  0.000000  0.00000
#> 3  0.000000  0.000000  0.000000  ...       0.57735  0.000000  0.57735
#> 
#> [4 rows x 22 columns]

cosine_similarity(tfidf_matrix, tfidf_matrix)
#> array([[1.       , 0.0754757, 0.       , 0.       ],
#>        [0.0754757, 1.       , 0.0819141, 0.       ],
#>        [0.       , 0.0819141, 1.       , 0.       ],
#>        [0.       , 0.       , 0.       , 1.       ]])
```


# Soft cosine similarity      

An obvious weakness of the cosine similarity matrix is that $n$ terms are arbitrarily assigned a dimension in ${\rm I\!R}^n$, regardless of similarities or differences in their semantics. Soft cosine similarity first implements word to vector embeddings, which allows terms with similar meanings be more closely localised together within the vector space.     

According to [Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure), to calculate the soft cosine, an additional matrix $s$ is used to indicate similarity between features, as calculated through the Levenshtein distance, WordNet similarity or other measures. In practice, we use pre-built word embedding models like `word2vec`, `fasttext` and others, which have been built by training large corpuses of publicly available text data.      

```{r, echo = FALSE, results = 'markup', fig.align = 'center', out.width = '80%', fig.cap = ''}
knitr::include_graphics("../02_figures/02_vectors-angles-revision-8.png")   
```


# Further reading   

+ A great [post](https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) explaining the maths behind vector dot products and cosine similarity.      

+ A [post](https://www.machinelearningplus.com/nlp/cosine-similarity/) explaining the different between cosine similarity and soft cosine similarity.    

+ A [guide](https://www.machinelearningplus.com/nlp/gensim-tutorial/) to the Python Gensim package, which is useful for creating word to vector embeddings.     

+ A [stack exchange post](https://stats.stackexchange.com/questions/31565/compute-a-cosine-dissimilarity-matrix-in-r) on how to calculate the cosine similarity matrix in R.   