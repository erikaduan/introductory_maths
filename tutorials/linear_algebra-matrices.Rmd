---
title: Introduction to matrices     
author: Erika Duan
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
    toc_depth: 2
    math_method:
      engine: webtex
      url: https://latex.codecogs.com/svg.format?
---

```{r setup, include=FALSE}
# Set up global environment configuration --------------------------------------
knitr::opts_chunk$set(echo=TRUE,
                      results='hide',
                      fig.show='hold',
                      fig.align='center',
                      message=FALSE,
                      warning=FALSE,
                      out.width='80%')

knitr::knit_engines$set(python = reticulate::eng_python)
```

```{r, echo=FALSE}
# Check version of Python used by reticulate -----------------------------------
reticulate::py_config()
```


# Matrices      

A matrix with $m$ rows and $n$ columns can be used to hold the coefficients from any linear system $A\vec x = \vec b$.   

If A is a matrix of linearly independent vectors $\vec a_1, \vec a_2, \cdots, \vec a_n$, each column vector represents a scalable basis vector with dimensions $m \times 1$. $Span\{\vec a_1, \vec a_2, \cdots, \vec a_n\}$ generates subspace H, where subspace H has $n$ dimensions and $\in \mathbb{R}^m$.        

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_transformations.svg")
```

**Note:** If matrix A is a square $n \times n$ matrix of linearly independent vectors, subspace H spanned by $\vec a_1, \vec a_2, \cdots, \vec a_n$ has n dimensions and $\in \mathbb{R}^n$.   


# Types of matrices   

Types of matrices include:  

+ Zero matrix: a matrix in which every entry is 0.   
+ Symmetrical matrix: a matrix where values on either side of the diagonal are equal to each other. 
+ Transpose matrix: If matrix A has dimensions $m\times n$, then its transpose matrix $A^T$ has dimensions $n \times m$ and every row of A is a column in $A^T$. Symmetrical matrices therefore have the property $A^T = A$.   
+ Identity matrix: a matrix with dimensions $m \times m$ where each diagonal entry is 1 and all other entries are 0. Identity matrices therefore have the property $IA = AI = A$.   

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_types.svg")
```


# Matrix column space   

For any $m\times n$ matrix A, $A = \begin{bmatrix} \vec a_1 & \vec a_2 & \cdots & \vec a_n \end{bmatrix}$. The column space of A, denoted as $ColA$, is the span of $\{\vec a_1, \vec a_2, \cdots, \vec a_n\}$.     

Therefore $ColA = \{\vec b \in \mathbb{R}^m | A\vec x = \vec b$ for some $\vec x \in \mathbb{R}^n\}$.    

If $\{\vec a_1, \vec a_2, \cdots, \vec a_n\}$ is linearly independent, $ColA$ is also a subspace of $\mathbb{R}^m$ with n-dimensions i.e. $ColA$ can be represented in matrix form with n pivot columns. If the matrix form of $ColA$ contains all pivot columns and therefore no free variables, $A\vec x = \vec b$ must have a single solution and $A\vec x = \vec 0$ only has the trivial solution.    

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_column_space.svg")
```


# Matrix null space   

For any $m\times n$ matrix A, the null space of A, denoted as $NulA$, is specifically defined as the set of all solutions to the homogeneous linear system $A\vec x=\vec 0$.  

Therefore $NulA = \{\vec x \in \mathbb{R}^n | A\vec x = \vec 0\}$.   

As homogeneous linear systems have either a single trivial solution or infinite solutions, $NulA$ is either $\vec 0$ or a subspace of $\mathbb{R}^n$.   

When $A\vec x=\vec 0$ has infinite solutions, $NulA$ can also be expressed through its parametric form $c_1\vec v_1 + c_2\vec v_2 + \cdots + c_n\vec v_h$ as $Span\{\vec v_1, \vec v_2, \cdots, \vec v_h\}$.    

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_null_space.svg")
```

In summary:   

+ $ColA$ represents the span of the basis vectors of matrix A. This is also called matrix rank.   
+ $NulA$ represents the set of all possible solutions to $A \vec x = \vec 0$. When the solution is presented in parametric form (when there are infinite solutions to $A\vec x = \vec 0$), $NulA$ also has a vector span in relation to its free variables. This is also called matrix nullity.    

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_rank_and_nullity.svg")
```

**Note:** From examples of homogeneous linear systems, we can see that the number of column vectors in matrix A, represented as $n$, is equal to the sum of the dimensions of $ColA$ and the dimensions of $NulA$.    


# Matrix scalar multiplication   

Matrix scalar multiplication is useful for mapping linear transformations, which is covered in the next tutorial on [linear transformations](https://github.com/erikaduan/introductory_maths/blob/master/tutorials/linear_algebra-linear_transformations.md).    

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_scalar_multiplication.svg")
```


# Matrix addition   

The most useful properties of matrix addition are:    

+ That $A + B = B + A$. This shows that the order of matrix addition does not matter.  
+ That $k(A + B) = kA + kB$. This shows that the scalar transformation of the sum of A and B is identical to the sum of the scalar transformation of A and the scalar transformation of B. This is also crucial for understanding the form of a linear transformation.    

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_addition.svg")
```


# Matrix multiplication   

Unlike matrix addition, the order of matrix multiplication impacts the matrix multiplication product and $A \times B \neq B \times A$.   

When A and B have the same dimensions, matrix multiplication represents the linear transformation of the original basis vectors (from A) onto the position of new basis vectors (from B), to form a new linear transformation (denoted by C) as interpreted using the standard coordinate system.   

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-matrix_multiplication.svg")
```


# The inverse matrix     

The matrix inverse can be thought of as a matrix form of the multiplication inverse $\tfrac{1}{k}$ where $k \times \tfrac{1}{k} = 1$. When finding inverse matrices, we only consider matrices with dimensions $n \times n$.    

A matrix with dimensions $n \times n$ is invertible if it has an inverse form such that $A\times A^{-1} = I$, where $I$ is the identity matrix.  

The existence of an inverse matrix $A^{-1}$ implies the following other properties:   

+ If $A\times A^{-1} = I$, $A^{-1}\times A = I$ is also true and therefor the inverse of $A^{-1}$ is A. Therefore $(A^{-1})^{-1} = A$.  
+ If matrices $M_1, M_2, \cdots, M_p$ are invertible $n \times n$ matrices, then $(M_1M_2\cdots M_p)(M_p^{-1}\cdots M_2^{-1}M_1^{-1}) = I$ and therefore the inverse of $M_1, M_2, \cdots, M_p$ can be written as $(M_1, M_2, \cdots, M_p)^{-1} = (M_p^{-1}\cdots M_2^{-1}M_1^{-1})$.   
+ If A is invertible, $A^T(A^{-1}){^T}= (A^{-1}A)^{T} = I^T = I$ and $(A^{-1}){^T}A^T= (AA^{-1})^{T} = I^T = I$. Therefore $A^T$ is also invertible and $(A^T)^{-1} = (A^{-1})^{T}$.

To develop an algorithm to find $A^{-1}$, we first consider the scenario where a single elementary row operation (ERO) is performed on the identity matrix to convert it into another row equivalent matrix.     

The ERO can be represented as matrix multiplication by the elementary matrix $E_1$ where $E_1I = E_1$. Since EROs are reversible, $E_1^{-1}$ also exists and $E_1E_1^{-1} = I$. The same ERO that transforms $I$ into $E_1$ can also be applied to matrix A to transform it into matrix B i.e. $E_1A = B$.      

If a single ERO transforms A into the identity matrix i.e. $E_1A=I$, then $E_1 = A^{-1}$. If a finite sequence of EROs transforms A into the identity matrix i.e. $(E_p\cdots E_2E_1)A=I$, then $(E_p\cdots E_2E_1)I = A^{-1}$.     

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-elementary_matrix.svg")   
```

Therefore a matrix is invertible if matrix $A$ is row equivalent to its identify matrix $I$ and any finite sequence of elementary row operations that transforms $A$ to $I$ also transforms $I$ to $A^{-1}$.      

Thus, the connection between linear systems and invertible matrices is that the linear system $A\vec x = \vec b$ only has a unique solution if matrix A is invertible, as the reduced echelon form of A is the identity matrix. We can therefore also solve for $\vec x$ using $\vec x = A^{-1} \vec b$.    

In the algorithm for finding the inverse matrix $A^{-1}$, we aim to:  

1. Write down the augmented matrix $\left[\begin{array}{c|c}A&I_n\end{array}\right]$.      
2. Row reduce the augmented matrix until its left-hand side is in reduced echelon form. Let this be the result $\left[\begin{array}{c|c}B&C\end{array}\right]$.    
3. If $B = I_n$, then the right-hand side of the augmented matrix is the inverse matrix i.e. $C=A^{-1}$. If the left-hand side cannot be simplified to a reduced echelon form, then matrix A is not invertible.   

```{r, echo=FALSE, results='markup'}
knitr::include_graphics("../figures/linear_systems-inverse_matrix.svg")
```

**Note:** If matrix A is row equivalent to the identity matrix $I_n$, then the columns of A must all contain pivot columns i.e. no free variables exist and the equation $A\vec x = \vec 0$  must only contain the trivial solution $x_1 = x_2 = \cdots = x_n = 0$.    

**Note:** A matrix that is not invertible is also called a singular matrix and an invertible matrix is therefore also called a non-singular matrix.    


# Matrix determinant  

# TODO


# Resources   
+ A great [YouTube video](https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3) introducing matrices by 3Blue1Brown.  
+ A great [YouTube video](https://www.youtube.com/watch?v=XkY2DOUCWMU&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=4) explaining the purpose of matrix multiplication by 3Blue1Brown.   
+ A [clear explanation](https://math.stackexchange.com/questions/664594/why-mathbf0-has-dimension-zero) of why the set containing only the zero vector has 0 dimensions.   